{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c7407d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.10.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "344e078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import random\n",
    "from lxml import html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f5619f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_robust_session():\n",
    "    session = requests.Session()\n",
    "    \n",
    "    # Configure retry strategy\n",
    "    retries = Retry(\n",
    "        total=5,  # number of retries\n",
    "        backoff_factor=0.5,  # wait 0.5, 1, 2, 4... seconds between retries\n",
    "        status_forcelist=[500, 502, 503, 504, 404, 403],\n",
    "    )\n",
    "    \n",
    "    # Mount the adapter with retry strategy\n",
    "    adapter = HTTPAdapter(max_retries=retries)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    \n",
    "    # Set multiple rotating User-Agents\n",
    "    user_agents = [\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',\n",
    "        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.1 Safari/605.1.15',\n",
    "        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.59'\n",
    "    ]\n",
    "    \n",
    "    session.headers.update({\n",
    "        'User-Agent': random.choice(user_agents),\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Accept-Encoding': 'gzip, deflate, br',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'Cache-Control': 'max-age=0'\n",
    "    })\n",
    "    \n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f7b9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to safely make requests\n",
    "def safe_get(url, session=None, max_retries=3):\n",
    "    if session is None:\n",
    "        session = create_robust_session()\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Add random delay between requests\n",
    "            time.sleep(random.uniform(1, 3))\n",
    "            \n",
    "            response = session.get(url)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "            \n",
    "            print(f\"Successfully retrieved page on attempt {attempt + 1}\")\n",
    "            return response\n",
    "            \n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            print(f\"Connection error on attempt {attempt + 1}: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            time.sleep(random.uniform(5, 10))  # Longer delay between retries\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request error on attempt {attempt + 1}: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            time.sleep(random.uniform(5, 10))\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db6ecf9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract list items\n",
    "def extract_list_items(response):\n",
    "    \"\"\"Extract href attributes from the specified xpath using both lxml and BeautifulSoup for verification.\"\"\"\n",
    "    results = {\n",
    "        'lxml': [],\n",
    "        'bs4': []\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Parse with lxml\n",
    "        tree = html.fromstring(response.content)\n",
    "        # XPath to select all relevant <a> tags under each <li>\n",
    "        list_items = tree.xpath('/html/body/div[1]/div/div[2]/main/div/div/div[2]/div[1]/ul/li//h2/a')\n",
    "        \n",
    "        print(f\"Found {len(list_items)} items using lxml\")\n",
    "        \n",
    "        for item in list_items:\n",
    "            href = item.get(\"href\")\n",
    "            if href:\n",
    "                results['lxml'].append(href)\n",
    "                print(f\"Found href with lxml: {href}\")\n",
    "        \n",
    "        # Parse with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'lxml')\n",
    "        ul_element = soup.select_one('div:nth-child(1) > ul')\n",
    "        \n",
    "        if ul_element:\n",
    "            # CSS selector to match the same path in BeautifulSoup\n",
    "            bs4_items = ul_element.select('li h2 a')\n",
    "            print(f\"Found {len(bs4_items)} items using BeautifulSoup\")\n",
    "            \n",
    "            for item in bs4_items:\n",
    "                href = item.get(\"href\")\n",
    "                if href:\n",
    "                    results['bs4'].append(href)\n",
    "                    print(f\"Found href with BeautifulSoup: {href}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in extraction: {e}\")\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f920d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fourth cell - Main execution\n",
    "def main():\n",
    "    url = \"https://www.edmunds.com/volkswagen/beetle/2012/\"\n",
    "    \n",
    "    try:\n",
    "        # Create session and get page\n",
    "        session = create_robust_session()\n",
    "        response = safe_get(url, session)\n",
    "        \n",
    "        if response and response.status_code == 200:\n",
    "            # Extract data\n",
    "            results = extract_list_items(response)\n",
    "            \n",
    "            # Print results from lxml\n",
    "            print(\"\\nResults from lxml:\")\n",
    "            print(\"------------------\")\n",
    "            for idx, href in enumerate(results['lxml'], 1):\n",
    "                print(f\"{idx}. URL: {href}\")\n",
    "            \n",
    "            # Print results from BeautifulSoup\n",
    "            print(\"\\nResults from BeautifulSoup:\")\n",
    "            print(\"-------------------------\")\n",
    "            for idx, href in enumerate(results['bs4'], 1):\n",
    "                print(f\"{idx}. URL: {href}\")\n",
    "            \n",
    "            # Create DataFrame for results\n",
    "            df_lxml = pd.DataFrame(results['lxml'], columns=['URL'])\n",
    "            df_bs4 = pd.DataFrame(results['bs4'], columns=['URL'])\n",
    "            \n",
    "            print(df_lxml)\n",
    "            print(df_bs4)\n",
    "            \n",
    "            # Save to CSV\n",
    "            #df_lxml.to_csv('vw_beetle_urls_lxml.csv', index=False)\n",
    "            #df_bs4.to_csv('vw_beetle_urls_bs4.csv', index=False)\n",
    "            \n",
    "            #print(\"\\nData saved to CSV files\")\n",
    "            \n",
    "            # Compare results\n",
    "            #print(\"\\nComparison of results:\")\n",
    "            #print(f\"LXML found {len(results['lxml'])} URLs\")\n",
    "            #print(f\"BeautifulSoup found {len(results['bs4'])} URLs\")\n",
    "            \n",
    "        else:\n",
    "            print(\"Failed to retrieve the page\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e76c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fifth cell - Run the script\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2095b419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
